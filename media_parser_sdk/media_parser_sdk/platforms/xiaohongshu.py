#!/usr/bin/env python3
"""
å°çº¢ä¹¦å¹³å°è§£æå™¨
"""

import re
import json
from typing import Optional
import httpx

from ..core.base_parser import BaseParser
from ..models.media_info import MediaInfo, MediaType, Platform, DownloadUrls
from ..exceptions import ParseError, NetworkError


class XiaohongshuParser(BaseParser):
    """å°çº¢ä¹¦å¹³å°è§£æå™¨"""

    def __init__(self, logger=None, cookie: str = None):
        """
        åˆå§‹åŒ–å°çº¢ä¹¦è§£æå™¨

        Args:
            logger: æ—¥å¿—è®°å½•å™¨
            cookie: å°çº¢ä¹¦Cookieï¼ˆå¯é€‰ï¼Œæœ‰åŠ©äºæé«˜è§£ææˆåŠŸç‡ï¼Œå°¤å…¶æ˜¯å®å†µå›¾ç‰‡ï¼‰
        """
        super().__init__(logger)
        self.cookie = cookie
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.xiaohongshu.com/",
            "Accept-Language": "zh-CN,zh;q=0.9"
        }
        if cookie:
            self.headers["Cookie"] = cookie
    
    def is_supported_url(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ”¯æŒè¯¥URL"""
        return any(domain in url.lower() for domain in ["xiaohongshu.com", "xhslink.com"])
    
    def parse(self, url: str) -> Optional[MediaInfo]:
        """è§£æå°çº¢ä¹¦é“¾æ¥"""
        try:
            self.validate_url(url)
            
            # è·å–ç½‘é¡µHTML
            html = self._get_html(url)
            
            # æå–åª’ä½“ä¿¡æ¯
            media_data = self._extract_media_info(html)
            if not media_data:
                raise ParseError("æ— æ³•æå–åª’ä½“ä¿¡æ¯", url=url, platform="xiaohongshu")
            
            # è·å–ä¸‹è½½é“¾æ¥
            download_urls = self._get_download_urls(media_data)
            
            # æ„å»ºMediaInfoå¯¹è±¡
            media_info = MediaInfo(
                platform=Platform.XIAOHONGSHU,
                title=media_data.get("title", "å°çº¢ä¹¦ç¬”è®°"),
                author=media_data.get("author", "æœªçŸ¥ä½œè€…"),
                media_type=self._determine_media_type(media_data, download_urls),
                note_id=media_data.get("note_id"),
                download_urls=download_urls,
                description=media_data.get("description"),
                tags=media_data.get("tags", []),
                has_live_photo=media_data.get("has_live_photo", False),
                raw_data=media_data.get("raw_data", {}),
                # ç»Ÿè®¡æ•°æ®
                like_count=media_data.get("like_count"),
                collect_count=media_data.get("collect_count"),
                comment_count=media_data.get("comment_count"),
                share_count=media_data.get("share_count"),
                view_count=media_data.get("view_count"),
                # å‘å¸ƒæ—¶é—´
                publish_time=media_data.get("publish_time"),
                url=url
            )
            
            return media_info
            
        except NetworkError as e:
            raise e
        except ParseError as e:
            raise e
        except Exception as e:
            raise ParseError(f"å°çº¢ä¹¦é“¾æ¥è§£æå¤±è´¥: {str(e)}", url=url, platform="xiaohongshu")
    
    def _get_html(self, url: str) -> str:
        """è·å–ç½‘é¡µHTMLå†…å®¹"""
        try:
            # ä¿ç•™åŸå§‹ URL çš„æ‰€æœ‰å‚æ•°
            self.log_debug(f"è¯·æ±‚ URL: {url}")
            self.log_debug(f"Cookie çŠ¶æ€: {'å·²æä¾›' if self.cookie else 'æœªæä¾›'}")

            # å¢å¼ºè¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
            enhanced_headers = self.headers.copy()
            enhanced_headers.update({
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Encoding": "gzip, deflate, br",
                "Cache-Control": "max-age=0",
                "Sec-Ch-Ua": '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                "Sec-Ch-Ua-Mobile": "?0",
                "Sec-Ch-Ua-Platform": '"macOS"',
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Upgrade-Insecure-Requests": "1",
            })

            with httpx.Client(headers=enhanced_headers, timeout=15, follow_redirects=True) as client:
                response = client.get(url)

                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 403:
                    self.log_warning("æ”¶åˆ° 403 ç¦æ­¢è®¿é—®ï¼Œå¯èƒ½éœ€è¦æœ‰æ•ˆçš„ Cookie")
                elif response.status_code == 404:
                    self.log_warning("é¡µé¢ä¸å­˜åœ¨æˆ–éœ€è¦ç™»å½•")

                response.raise_for_status()

                # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢
                html_lower = response.text.lower()
                if "ä½ è®¿é—®çš„é¡µé¢ä¸è§äº†" in response.text or "é¡µé¢æ‰¾ä¸åˆ°" in response.text:
                    self.log_error("è·å–åˆ°é”™è¯¯é¡µé¢ï¼šé¡µé¢ä¸å­˜åœ¨æˆ–éœ€è¦ç™»å½•")
                    if not self.cookie:
                        self.log_warning("ğŸ’¡ æç¤ºï¼šæä¾› Cookie å¯èƒ½èƒ½è§£å†³è¿™ä¸ªé—®é¢˜")
                        self.log_warning("   è·å–æ–¹å¼ï¼šæµè§ˆå™¨ F12 -> Network -> å¤åˆ¶ Request Header ä¸­çš„ Cookie")
                    raise NetworkError(f"é¡µé¢è®¿é—®å—é™ï¼ˆå¯èƒ½éœ€è¦æä¾› Cookieï¼‰", url=url)

                return response.text
        except httpx.HTTPError as e:
            self.log_error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
            raise NetworkError(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}", url=url)
    
    def _extract_media_info(self, html: str) -> Optional[dict]:
        """ä»HTMLä¸­æå–åª’ä½“ä¿¡æ¯"""
        try:
            media_data = {
                "title": "å°çº¢ä¹¦ç¬”è®°",
                "author": "æœªçŸ¥ä½œè€…",
                "note_id": None,
                "has_live_photo": False,
                "raw_data": {}
            }
            
            # æå–window.__INITIAL_STATE__è„šæœ¬æ•°æ®
            initial_state_pattern = re.compile(r'window\.__INITIAL_STATE__\s*=\s*(.+?)(?=</script>)', re.DOTALL)
            initial_state_match = initial_state_pattern.search(html)
            
            if initial_state_match:
                initial_state_str = initial_state_match.group(1).strip()
                if initial_state_str.endswith(';'):
                    initial_state_str = initial_state_str[:-1]
                
                try:
                    initial_state = json.loads(initial_state_str)
                    media_data["raw_data"] = initial_state
                    
                    if self._parse_initial_state(initial_state, media_data):
                        return media_data
                except json.JSONDecodeError as e:
                    self.log_debug(f"__INITIAL_STATE__è§£æå¤±è´¥: {str(e)}")
                    
                    # å°è¯•ä¿®å¤JSONè§£æé—®é¢˜
                    try:
                        fixed_str = re.sub(r'\bundefined\b', 'null', initial_state_str)
                        fixed_str = re.sub(r',(\s*[}\]])', r'\1', fixed_str)
                        fixed_str = re.sub(r'//.*?\n', '\n', fixed_str)
                        fixed_str = re.sub(r'/\*.*?\*/', '', fixed_str, flags=re.DOTALL)
                        
                        initial_state = json.loads(fixed_str)
                        media_data["raw_data"] = initial_state
                        
                        if self._parse_initial_state(initial_state, media_data):
                            return media_data
                    except json.JSONDecodeError as e2:
                        self.log_debug(f"ä¿®å¤åä»ç„¶è§£æå¤±è´¥: {str(e2)}")
            
            # å¤‡ç”¨æ–¹æ³•ï¼šé€šè¿‡metaæ ‡ç­¾åˆ†æ
            title_match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)
            if title_match:
                media_data["title"] = title_match.group(1).replace(" - å°çº¢ä¹¦", "")

            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢
            if "ä½ è®¿é—®çš„é¡µé¢ä¸è§äº†" in html or "é¡µé¢æ‰¾ä¸åˆ°" in html:
                self.log_error("è·å–åˆ°é”™è¯¯é¡µé¢ï¼šé¡µé¢ä¸å­˜åœ¨æˆ–éœ€è¦ç™»å½•")
                self.log_warning(f"é¡µé¢æ ‡é¢˜: {media_data.get('title')}")
                if not self.cookie:
                    self.log_warning("ğŸ’¡ æç¤ºï¼šæä¾› Cookie å¯èƒ½èƒ½è§£å†³è¿™ä¸ªé—®é¢˜")
                    self.log_warning("   è·å–æ–¹å¼ï¼šæµè§ˆå™¨ F12 -> Network -> å¤åˆ¶ Request Header ä¸­çš„ Cookie")
                    self.log_warning("   ä½¿ç”¨æ–¹å¼ï¼šXiaohongshuParser(cookie='ä½ çš„Cookie')")

            return media_data
            
        except Exception as e:
            self.log_error(f"æå–åª’ä½“ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
    
    def _parse_initial_state(self, initial_state: dict, media_data: dict) -> bool:
        """ä»__INITIAL_STATE__ä¸­è§£æè¯¦ç»†åª’ä½“ä¿¡æ¯"""
        try:
            note = initial_state.get("note", {})
            note_detail_map = note.get("noteDetailMap", {})
            
            if note_detail_map:
                note_id = next(iter(note_detail_map.keys()), None)
                if note_id:
                    note_detail = note_detail_map[note_id]
                    note_data = note_detail.get("note", {})
                    
                    if note_data:
                        media_data["title"] = note_data.get("title", media_data["title"])
                        media_data["note_id"] = note_data.get("noteId", note_id)
                        media_data["description"] = note_data.get("desc", "")
                        
                        # æå–ç”¨æˆ·ä¿¡æ¯
                        user_data = note_data.get("user", {})
                        if isinstance(user_data, dict):
                            media_data["author"] = user_data.get("nickname", media_data["author"])
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰å®å†µå›¾ç‰‡ï¼ˆå¢å¼ºé€»è¾‘ï¼‰
                        media_data["has_live_photo"] = False
                        image_list = note_data.get("imageList", [])
                        if image_list:
                            for img in image_list:
                                # æ£€æŸ¥å¤šç§å®å†µå›¾ç‰‡çš„è¡¨ç¤ºæ–¹å¼
                                if img.get("livePhoto") or img.get("live_photo") or img.get("livephoto"):
                                    media_data["has_live_photo"] = True
                                    break
                        
                        # æå–æ ‡ç­¾
                        tag_list = note_data.get("tagList", [])
                        media_data["tags"] = [tag.get("name", "") for tag in tag_list if tag.get("name")]

                        # æå–ç»Ÿè®¡æ•°æ® - å°çº¢ä¹¦æ•°æ®åœ¨ interactInfo å¯¹è±¡ä¸­
                        interact_info = note_data.get("interactInfo", {})

                        # ä» interactInfo ä¸­æå–ï¼ˆæ–°ç‰ˆæœ¬æ•°æ®ç»“æ„ï¼‰
                        if interact_info:
                            # å°è¯•ä» interactInfo è·å–ï¼Œå€¼å¯èƒ½æ˜¯å­—ç¬¦ä¸²éœ€è¦è½¬æ¢ä¸ºæ•´æ•°
                            media_data["like_count"] = self._safe_int(interact_info.get("likedCount"))
                            media_data["collect_count"] = self._safe_int(interact_info.get("collectedCount"))
                            media_data["comment_count"] = self._safe_int(interact_info.get("commentCount"))
                            media_data["share_count"] = self._safe_int(interact_info.get("shareCount"))
                        else:
                            # å¤‡ç”¨ï¼šä» note_data æ ¹çº§åˆ«è·å–ï¼ˆæ—§ç‰ˆæœ¬æ•°æ®ç»“æ„ï¼‰
                            media_data["like_count"] = self._safe_int(note_data.get("likedCount") or note_data.get("like_count"))
                            media_data["collect_count"] = self._safe_int(note_data.get("collectedCount") or note_data.get("collect_count"))
                            media_data["comment_count"] = self._safe_int(note_data.get("commentCount") or note_data.get("comment_count"))
                            media_data["share_count"] = self._safe_int(note_data.get("shareCount") or note_data.get("share_count"))

                        # viewCount é€šå¸¸ä¸åœ¨ interactInfo ä¸­ï¼Œä»æ ¹çº§åˆ«è·å–
                        media_data["view_count"] = self._safe_int(note_data.get("viewCount") or note_data.get("view_count"))

                        # è®°å½•æå–çš„ç»Ÿè®¡æ•°æ®
                        self.log_info(f"æå–ç»Ÿè®¡æ•°æ® - ç‚¹èµ:{media_data['like_count']}, æ”¶è—:{media_data['collect_count']}, è¯„è®º:{media_data['comment_count']}")

                        # æå–å‘å¸ƒæ—¶é—´ï¼ˆå°çº¢ä¹¦ä½¿ç”¨æ¯«ç§’çº§æ—¶é—´æˆ³ï¼‰
                        publish_time = note_data.get("time") or note_data.get("publishTime") or note_data.get("publish_time")
                        if publish_time:
                            try:
                                from datetime import datetime
                                if isinstance(publish_time, (int, float)):
                                    # æ¯«ç§’çº§æ—¶é—´æˆ³è½¬æ¢
                                    media_data["publish_time"] = datetime.fromtimestamp(publish_time / 1000)
                                    self.log_info(f"æå–å‘å¸ƒæ—¶é—´: {media_data['publish_time']}")
                                elif isinstance(publish_time, str):
                                    # ISOæ ¼å¼å­—ç¬¦ä¸²
                                    media_data["publish_time"] = datetime.fromisoformat(publish_time.replace('Z', '+00:00'))
                                    self.log_info(f"æå–å‘å¸ƒæ—¶é—´: {media_data['publish_time']}")
                            except Exception as e:
                                self.log_debug(f"å‘å¸ƒæ—¶é—´è§£æå¤±è´¥: {publish_time}, é”™è¯¯: {e}")

                        # ä¿å­˜å®Œæ•´çš„noteæ•°æ®ï¼Œç”¨äºåç»­ä¸‹è½½é“¾æ¥æå–
                        media_data["note_data"] = note_data
                        return True
            
            # å¤‡é€‰æ–¹æ¡ˆï¼šæ£€æŸ¥å…¶ä»–å¯èƒ½çš„noteæ•°æ®ä½ç½®
            try:
                # æ£€æŸ¥noteDetailMapçš„å…¶ä»–å¯èƒ½ç»“æ„
                if isinstance(initial_state, dict):
                    # éå†æ•´ä¸ªinitial_stateï¼Œå¯»æ‰¾å¯èƒ½çš„noteæ•°æ®
                    for key, value in initial_state.items():
                        if isinstance(value, dict):
                            if "imageList" in value:
                                # å¯èƒ½æ˜¯ç›´æ¥çš„noteæ•°æ®
                                media_data["note_data"] = value
                                # æ£€æŸ¥æ˜¯å¦æœ‰å®å†µå›¾ç‰‡
                                media_data["has_live_photo"] = False
                                image_list = value.get("imageList", [])
                                for img in image_list:
                                    if img.get("livePhoto") or img.get("live_photo") or img.get("livephoto"):
                                        media_data["has_live_photo"] = True
                                        break
                                return True
            except Exception as e:
                self.log_debug(f"å¤‡é€‰æ–¹æ¡ˆè§£æå¤±è´¥: {str(e)}")
            
            return False
        except Exception as e:
            self.log_debug(f"è§£æ__INITIAL_STATE__è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}")
            return False

    def _safe_int(self, value) -> int:
        """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•´æ•°"""
        if value is None:
            return 0
        if isinstance(value, int):
            return value
        if isinstance(value, str):
            try:
                return int(value)
            except ValueError:
                return 0
        if isinstance(value, float):
            return int(value)
        return 0

    def _get_download_urls(self, media_data: dict) -> DownloadUrls:
        """è·å–ä¸‹è½½é“¾æ¥"""
        download_urls = DownloadUrls()

        try:
            # ä»note_dataä¸­æå–åª’ä½“URL
            note_data = media_data.get("note_data")
            if note_data:
                self._extract_urls_from_note_data(note_data, download_urls)

            # ä»raw_dataä¸­æœç´¢æ‰€æœ‰å¯èƒ½çš„åª’ä½“é“¾æ¥
            raw_data = media_data.get("raw_data", {})
            if raw_data:
                self._extract_all_urls_from_data(raw_data, download_urls)

            # æœ€ç»ˆå»é‡ï¼šä½¿ç”¨è§„èŒƒåŒ–URLè¿›è¡Œå»é‡ï¼ˆå»é™¤æ‰€æœ‰æŸ¥è¯¢å‚æ•°ï¼‰
            download_urls.video = self._deduplicate_urls(download_urls.video)
            download_urls.images = self._deduplicate_urls(download_urls.images)
            download_urls.live = self._deduplicate_urls(download_urls.live)

            self.log_debug(f"å»é‡å: è§†é¢‘{len(download_urls.video)}ä¸ª, å›¾ç‰‡{len(download_urls.images)}å¼ , å®å†µ{len(download_urls.live)}ä¸ª")

            return download_urls

        except Exception as e:
            self.log_error(f"è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: {str(e)}")
            return download_urls

    def _deduplicate_urls(self, url_list: list) -> list:
        """URLå»é‡ - ä½¿ç”¨åŸºç¡€URLï¼ˆå»é™¤æŸ¥è¯¢å‚æ•°ï¼‰è¿›è¡Œæ¯”è¾ƒ"""
        seen = set()
        result = []

        for url in url_list:
            # æå–åŸºç¡€URLï¼ˆå»é™¤æŸ¥è¯¢å‚æ•°å’Œç‰‡æ®µï¼‰
            base_url = url.split('?')[0].split('#')[0]

            if base_url not in seen:
                seen.add(base_url)
                result.append(url)
            else:
                self.log_debug(f"å»é™¤é‡å¤URL: {url}")

        return result
    
    def _extract_urls_from_note_data(self, note_data: dict, download_urls: DownloadUrls) -> None:
        """ä»note_dataä¸­æå–åª’ä½“URL"""
        try:
            self.log_debug(f"å¼€å§‹ä»note_dataæå–URL")
            
            # å¤„ç†è§†é¢‘æ•°æ®
            if note_data.get("type") == "video" or "video" in note_data:
                video_data = note_data.get("video")
                if video_data:
                    self.log_debug(f"æ‰¾åˆ°è§†é¢‘æ•°æ®: {video_data.keys()}")
                    # æ”¯æŒæ–°è€ä¸¤ç§è§†é¢‘æ•°æ®ç»“æ„
                    h264_data = None
                    if "stream" in video_data:
                        h264_data = video_data.get("stream", {}).get("h264")
                    elif "media" in video_data:
                        h264_data = video_data.get("media", {}).get("stream", {}).get("h264")
                    elif "videoUrl" in video_data:
                        # ç›´æ¥çš„è§†é¢‘URLå­—æ®µ
                        video_url = video_data.get("videoUrl")
                        if video_url:
                            clean_url = self.clean_url(video_url)
                            download_urls.video.append(clean_url)
                            self.log_debug(f"æå–åˆ°ç›´æ¥è§†é¢‘URL: {clean_url}")
                    
                    if h264_data and isinstance(h264_data, list):
                        for h264_item in h264_data:
                            if isinstance(h264_item, dict):
                                master_url = h264_item.get("masterUrl")
                                if master_url:
                                    clean_url = self.clean_url(master_url)
                                    download_urls.video.append(clean_url)
                                    self.log_debug(f"æå–åˆ°H264è§†é¢‘URL: {clean_url}")
            
            # å¤„ç†å›¾ç‰‡æ•°æ®
            image_list = note_data.get("imageList")
            if image_list and isinstance(image_list, list):
                self.log_debug(f"æ‰¾åˆ°å›¾ç‰‡åˆ—è¡¨ï¼Œé•¿åº¦: {len(image_list)}")
                for i, image_item in enumerate(image_list):
                    if isinstance(image_item, dict):
                        self.log_debug(f"å¤„ç†å›¾ç‰‡ {i+1}: {list(image_item.keys())}")
                        
                        # æå–é™æ€å›¾ç‰‡URL
                        image_url = None
                        
                        # ä¼˜å…ˆä»infoListä¸­è·å–é«˜è´¨é‡å›¾ç‰‡
                        info_list = image_item.get("infoList", [])
                        if isinstance(info_list, list):
                            for info in info_list:
                                if isinstance(info, dict):
                                    scene = info.get("imageScene", "")
                                    url = info.get("url", "")
                                    if scene == "WB_DFT" and url:
                                        image_url = url
                                        break
                                    elif scene == "WB_PRV" and url and not image_url:
                                        image_url = url
                        
                        # å¤‡ç”¨å­—æ®µ
                        if not image_url:
                            image_url = (image_item.get("urlDefault") or 
                                        image_item.get("url") or 
                                        image_item.get("urlPre") or
                                        image_item.get("urlList", [{}])[0].get("url", ""))
                        
                        if image_url:
                            clean_url = self.clean_url(image_url)
                            download_urls.images.append(clean_url)
                            self.log_debug(f"æå–åˆ°å›¾ç‰‡URL: {clean_url}")
                        
                        # æå–å®å†µå›¾ç‰‡çš„è§†é¢‘URLï¼ˆå¢å¼ºé€»è¾‘ï¼‰
                        live_photo = image_item.get("livePhoto") or image_item.get("live_photo") or image_item.get("livephoto")

                        # æ–°å¢ï¼šæ£€æŸ¥æ›´å¤šå¯èƒ½çš„å­—æ®µ
                        if not live_photo:
                            # æ£€æŸ¥ infoList ä¸­æ˜¯å¦æœ‰å®å†µå›¾ç‰‡ä¿¡æ¯
                            info_list = image_item.get("infoList", [])
                            if info_list:
                                for info in info_list:
                                    if isinstance(info, dict):
                                        # æ£€æŸ¥å„ç§å¯èƒ½çš„å®å†µå›¾ç‰‡æ ‡è®°
                                        if info.get("livePhoto") or info.get("live_photo"):
                                            live_photo = info
                                            self.log_debug(f"ä» infoList æ‰¾åˆ°å®å†µå›¾ç‰‡æ•°æ®")
                                            break

                        if live_photo:
                            self.log_debug(f"æ‰¾åˆ°å®å†µå›¾ç‰‡æ•°æ®: {live_photo}")
                            if isinstance(live_photo, dict):
                                # å°è¯•å¤šç§å¯èƒ½çš„è§†é¢‘URLå­—æ®µ
                                video_url = (live_photo.get("videoUrl") or
                                            live_photo.get("video_url") or
                                            live_photo.get("url") or
                                            live_photo.get("video") or
                                            live_photo.get("media") or
                                            live_photo.get("stream"))

                                if video_url:
                                    # video_url å¯èƒ½æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œéœ€è¦è¿›ä¸€æ­¥å¤„ç†
                                    if isinstance(video_url, dict):
                                        self.log_debug(f"å®å†µå›¾ç‰‡URLæ˜¯å­—å…¸ç±»å‹ï¼Œå°è¯•æå–: {list(video_url.keys())}")
                                        # å°è¯•ä»å¯¹è±¡ä¸­æå–å®é™…çš„ URL
                                        video_url = (video_url.get("masterUrl") or
                                                    video_url.get("url") or
                                                    video_url.get("defaultUrl"))

                                    if video_url and isinstance(video_url, str):
                                        clean_url = self.clean_url(video_url)
                                        if clean_url not in download_urls.live:
                                            download_urls.live.append(clean_url)
                                            self.log_info(f"âœ“ æˆåŠŸæå–å®å†µå›¾ç‰‡URL: {clean_url}")
                                    else:
                                        self.log_debug(f"å®å†µå›¾ç‰‡URLä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹: {type(video_url)}, å€¼: {video_url}")
                                else:
                                    self.log_debug(f"livePhotoå¯¹è±¡ä¸­çš„å­—æ®µ: {list(live_photo.keys())}")
                            elif isinstance(live_photo, str):
                                # å®å†µå›¾ç‰‡å¯èƒ½ç›´æ¥æ˜¯å­—ç¬¦ä¸²URL
                                clean_url = self.clean_url(live_photo)
                                if clean_url not in download_urls.live:
                                    download_urls.live.append(clean_url)
                                    self.log_info(f"âœ“ æˆåŠŸæå–å®å†µå›¾ç‰‡URLï¼ˆå­—ç¬¦ä¸²ï¼‰: {clean_url}")
                        else:
                            # è°ƒè¯•ä¿¡æ¯ï¼šè®°å½•å›¾ç‰‡é¡¹çš„æ‰€æœ‰å­—æ®µï¼Œå¸®åŠ©è¯†åˆ«æ–°çš„æ•°æ®ç»“æ„
                            self.log_debug(f"å›¾ç‰‡é¡¹å­—æ®µ: {list(image_item.keys())}")
        
        except Exception as e:
            self.log_debug(f"ä»note_dataæå–URLå¤±è´¥: {str(e)}")
            import traceback
            self.log_debug(traceback.format_exc())
    
    def _extract_all_urls_from_data(self, data: dict, download_urls: DownloadUrls) -> None:
        """ä»æ•°æ®ä¸­æå–æ‰€æœ‰å¯èƒ½çš„åª’ä½“URL"""
        try:
            data_str = json.dumps(data)
            
            # åŒ¹é…æ‰€æœ‰åª’ä½“é“¾æ¥
            media_pattern = re.compile(r'"(https?://[^"]+?\.(mp4|jpg|png|webp|mov|gif)[^"]*)"')
            media_matches = media_pattern.findall(data_str)
            
            for match in media_matches:
                url = match[0]
                ext = match[1]
                
                clean_url = self.clean_url(url)
                
                # ç‰¹æ®Šå¤„ç†MOVæ ¼å¼ï¼Œé€šå¸¸æ˜¯å®å†µå›¾ç‰‡
                if ext == "mov":
                    if clean_url not in download_urls.live:
                        download_urls.live.append(clean_url)
                        self.log_debug(f"ä»raw_dataæå–åˆ°å®å†µå›¾ç‰‡URL: {clean_url}")
                elif ext == "mp4":
                    if clean_url not in download_urls.video:
                        download_urls.video.append(clean_url)
                        self.log_debug(f"ä»raw_dataæå–åˆ°è§†é¢‘URL: {clean_url}")
                else:
                    if clean_url not in download_urls.images:
                        download_urls.images.append(clean_url)
                        self.log_debug(f"ä»raw_dataæå–åˆ°å›¾ç‰‡URL: {clean_url}")
            
            # ä¸“é—¨æœç´¢livePhotoç›¸å…³çš„URL
            live_photo_pattern = re.compile(r'livePhoto[^\"]*"(https?://[^\"]+?\.(mov|mp4)[^\"]*)"', re.DOTALL | re.IGNORECASE)
            live_photo_matches = live_photo_pattern.findall(data_str)
            
            for match in live_photo_matches:
                url = match[0]
                ext = match[1]
                clean_url = self.clean_url(url)
                
                if clean_url not in download_urls.live:
                    download_urls.live.append(clean_url)
                    self.log_debug(f"ä»livePhotoç›¸å…³å†…å®¹æå–åˆ°å®å†µå›¾ç‰‡URL: {clean_url}")
        
        except Exception as e:
            self.log_debug(f"æå–æ‰€æœ‰URLå¤±è´¥: {str(e)}")
            import traceback
            self.log_debug(traceback.format_exc())
    
    def _determine_media_type(self, media_data: dict, download_urls: DownloadUrls) -> MediaType:
        """ç¡®å®šåª’ä½“ç±»å‹"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å®å†µå›¾ç‰‡
        if media_data.get("has_live_photo") or download_urls.live:
            return MediaType.LIVE_PHOTO
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘ï¼ˆä½†æ²¡æœ‰å®å†µå›¾ç‰‡ï¼‰
        if download_urls.video and not download_urls.images:
            return MediaType.VIDEO
        
        # é»˜è®¤ä¸ºå›¾ç‰‡ç±»å‹
        return MediaType.IMAGE