#!/usr/bin/env python3
"""
å°çº¢ä¹¦å¢å¼ºè§£æå™¨ - æ”¯æŒç”¨æˆ·ä¸»é¡µå’Œç¬”è®°é›†åˆåŠŸèƒ½
"""

import re
import json
import time
from typing import Optional, List, Dict, Any
from urllib.parse import urlparse, parse_qs, urlencode
from datetime import datetime

try:
    import httpx
except ImportError:
    httpx = None

from ..core.base_parser import BaseParser
from ..models.media_info import MediaInfo, MediaType, Platform, DownloadUrls
from ..exceptions import ParseError, NetworkError

# å¯¼å…¥æ•°æ®æ¨¡å‹
try:
    from ..models.xiaohongshu_models import (
        NoteInfo, AuthorInfo, AuthorProfile, AuthorNotesCollection,
        InteractionStats, MediaResource, VideoResource, NoteType, XiaohongshuExtractResult
    )
except ImportError:
    # ç®€åŒ–ç‰ˆæœ¬çš„æ•°æ®æ¨¡å‹
    from pydantic import BaseModel
    from enum import Enum

    class NoteType(str, Enum):
        NORMAL = "normal"
        VIDEO = "video"
        LIVE_PHOTO = "live_photo"
        CAROUSEL = "carousel"

    class XiaohongshuExtractResult(BaseModel):
        success: bool
        result_type: str
        data: Optional[Dict[str, Any]] = None
        error_message: Optional[str] = None


class XiaohongshuEnhancedParser(BaseParser):
    """å°çº¢ä¹¦å¢å¼ºè§£æå™¨ - æ”¯æŒç”¨æˆ·ä¸»é¡µå’Œç¬”è®°é›†åˆ

    æ³¨æ„ï¼šè·å–ç”¨æˆ·ä¸»é¡µçš„å®Œæ•´ç¬”è®°ä¿¡æ¯éœ€è¦æä¾› Cookie
    """

    def __init__(self, logger=None, cookie: str = None):
        """
        åˆå§‹åŒ–è§£æå™¨

        Args:
            logger: æ—¥å¿—è®°å½•å™¨
            cookie: å°çº¢ä¹¦ Cookieï¼Œç”¨äºè·å–å®Œæ•´çš„ç¬”è®°ä¿¡æ¯
                    è·å–æ–¹å¼ï¼šæµè§ˆå™¨å¼€å‘è€…å·¥å…· -> Network -> å¤åˆ¶ Cookie
        """
        super().__init__(logger)
        self.cookie = cookie
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.xiaohongshu.com/",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        }
        if cookie:
            self.headers["Cookie"] = cookie
            self.has_cookie = True
        else:
            self.has_cookie = False

        self.request_delay = 1.0
        self.base_url = "https://www.xiaohongshu.com"

        # å¯¼å…¥å•ä¸ªç¬”è®°è§£æå™¨ï¼Œç”¨äºè·å–ç¬”è®°è¯¦æƒ…
        try:
            from .xiaohongshu import XiaohongshuParser
            self.note_parser = XiaohongshuParser(logger=logger)
            if cookie:
                # å¦‚æœæä¾›äº† Cookieï¼Œä¹Ÿè®¾ç½®åˆ° note_parser ä¸­
                self.note_parser.headers = self.note_parser.headers.copy()
                self.note_parser.headers["Cookie"] = cookie
        except ImportError:
            self.note_parser = None
            self.log_warning("æ— æ³•å¯¼å…¥XiaohongshuParserï¼Œç¬”è®°è¯¦æƒ…åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")

    def is_supported_url(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ”¯æŒè¯¥URL"""
        return any(domain in url.lower() for domain in ["xiaohongshu.com", "xhslink.com"])

    def parse(self, url: str) -> Optional[MediaInfo]:
        """è§£æåª’ä½“é“¾æ¥ - BaseParseræŠ½è±¡æ–¹æ³•å®ç°"""
        # åˆ¤æ–­URLç±»å‹
        if "/user/profile/" in url or "/user/profile/" in url:
            # ç”¨æˆ·ä¸»é¡µ
            result = self.parse_author_profile_sync(url)
            if result.success and result.data:
                # è½¬æ¢ä¸º MediaInfoï¼ˆä½¿ç”¨ç¬¬ä¸€æ¡ç¬”è®°ä½œä¸ºä»£è¡¨ï¼‰
                notes = result.data.get("notes", [])
                if notes:
                    return self._note_to_media_info(notes[0], url)
        else:
            # å•ä¸ªç¬”è®°ï¼Œä½¿ç”¨é»˜è®¤è§£æå™¨
            if self.note_parser:
                return self.note_parser.parse(url)
        return None

    def parse_author_profile_sync(self, url: str) -> XiaohongshuExtractResult:
        """åŒæ­¥ç‰ˆæœ¬ï¼šæå–åšä¸»èµ„æ–™"""
        try:
            self.log_info(f"å¼€å§‹è§£æåšä¸»ä¸»é¡µ: {url}")

            # æå–ç”¨æˆ·ID
            user_id = self._extract_user_id_from_profile_url(url)
            if not user_id:
                return XiaohongshuExtractResult(
                    success=False,
                    result_type="author_profile",
                    error_message="æ— æ³•ä»URLä¸­æå–ç”¨æˆ·ID"
                )

            # è·å–ç”¨æˆ·ä¸»é¡µHTML
            html = self._get_html(url)
            if not html:
                return XiaohongshuExtractResult(
                    success=False,
                    result_type="author_profile",
                    error_message="æ— æ³•è·å–ç”¨æˆ·ä¸»é¡µå†…å®¹"
                )

            # è§£æç”¨æˆ·ä¿¡æ¯
            author_profile = self._parse_user_profile_html(html, user_id, url)
            if not author_profile:
                return XiaohongshuExtractResult(
                    success=False,
                    result_type="author_profile",
                    error_message="æ— æ³•è§£æç”¨æˆ·èµ„æ–™"
                )

            return XiaohongshuExtractResult(
                success=True,
                result_type="author_profile",
                data=author_profile
            )

        except Exception as e:
            self.log_error(f"è§£æåšä¸»ä¸»é¡µå¤±è´¥: {str(e)}")
            return XiaohongshuExtractResult(
                success=False,
                result_type="author_profile",
                error_message=str(e)
            )

    def parse_author_notes_sync(
        self,
        url: str,
        max_notes: Optional[int] = None,
        fetch_detail: bool = True
    ) -> XiaohongshuExtractResult:
        """
        åŒæ­¥ç‰ˆæœ¬ï¼šæå–åšä¸»æ‰€æœ‰ç¬”è®°

        Args:
            url: ç”¨æˆ·ä¸»é¡µURL
            max_notes: æœ€å¤§æå–ç¬”è®°æ•°ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            fetch_detail: æ˜¯å¦è·å–æ¯æ¡ç¬”è®°çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬é«˜æ¸…å›¾ç‰‡/è§†é¢‘ï¼‰

        æ³¨æ„ï¼š
            - è·å–å®Œæ•´çš„ç¬”è®°ä¿¡æ¯éœ€è¦æä¾› Cookie
            - æ—  Cookie æ—¶åªèƒ½è·å–ç¬”è®°å¡ç‰‡ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€å°é¢ã€ç‚¹èµæ•°ç­‰ï¼‰
            - æœ‰ Cookie æ—¶å¯ä»¥è·å–å®Œæ•´çš„é«˜æ¸…å›¾ç‰‡å’Œè§†é¢‘ä¸‹è½½é“¾æ¥
        """
        try:
            # Cookie æç¤º
            if fetch_detail and not self.has_cookie:
                self.log_warning("âš ï¸  æœªæä¾› Cookieï¼Œæ— æ³•è·å–å®Œæ•´çš„ç¬”è®°ä¿¡æ¯ï¼ˆé«˜æ¸…å›¾ç‰‡ã€è§†é¢‘ä¸‹è½½é“¾æ¥ï¼‰")
                self.log_warning("âš ï¸  å°†ä»…è¿”å›ç¬”è®°å¡ç‰‡ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€å°é¢ã€ç‚¹èµæ•°ï¼‰")
                self.log_warning("ğŸ’¡ æç¤ºï¼šæä¾› Cookie å¯è·å–å®Œæ•´ä¿¡æ¯")
                self.log_warning("   è·å–æ–¹å¼ï¼šæµè§ˆå™¨ F12 -> Network -> å¤åˆ¶ Request Header ä¸­çš„ Cookie")
                self.log_warning("   ä½¿ç”¨æ–¹å¼ï¼šparse_author_notes_sync(url, cookie='ä½ çš„Cookie')")

            self.log_info(f"å¼€å§‹æå–åšä¸»ç¬”è®°: {url}, max_notes={max_notes}, fetch_detail={fetch_detail}")
            self.log_info(f"Cookie çŠ¶æ€: {'âœ“ å·²æä¾›' if self.has_cookie else 'âœ— æœªæä¾›'}")

            # æå–ç”¨æˆ·ID
            user_id = self._extract_user_id_from_profile_url(url)
            if not user_id:
                return XiaohongshuExtractResult(
                    success=False,
                    result_type="author_notes",
                    error_message="æ— æ³•ä»URLä¸­æå–ç”¨æˆ·ID"
                )

            # è·å–ç”¨æˆ·ä¸»é¡µHTML
            html = self._get_html(url)
            if not html:
                return XiaohongshuExtractResult(
                    success=False,
                    result_type="author_notes",
                    error_message="æ— æ³•è·å–ç”¨æˆ·ä¸»é¡µå†…å®¹"
                )

            # è§£æç”¨æˆ·ä¿¡æ¯å’Œç¬”è®°å¡ç‰‡
            user_data = self._parse_user_page_html(html, user_id, url)
            if not user_data:
                return XiaohongshuExtractResult(
                    success=False,
                    result_type="author_notes",
                    error_message="æ— æ³•è§£æç”¨æˆ·é¡µé¢"
                )

            note_cards = user_data.get("note_cards", [])
            author_info = user_data.get("author_info", {})
            total_notes_count = user_data.get("total_notes_count", len(note_cards))

            self.log_info(f"æ‰¾åˆ° {len(note_cards)} æ¡ç¬”è®°å¡ç‰‡ï¼Œæ€»æ•°: {total_notes_count}")

            # å¦‚æœéœ€è¦è·å–è¯¦ç»†ä¿¡æ¯ï¼Œä½¿ç”¨å•ä¸ªç¬”è®°è§£æå™¨
            detailed_notes = []
            extracted_count = 0

            for i, card in enumerate(note_cards):
                if max_notes and extracted_count >= max_notes:
                    break

                note_id = card.get("note_id")
                self.log_info(f"å¤„ç†ç¬”è®° {i+1}/{len(note_cards)}: {note_id or 'æ— ID'}")

                if fetch_detail and note_id:
                    # æ„é€ ç¬”è®°URLå¹¶è§£æ
                    note_url = f"{self.base_url}/explore/{note_id}"
                    try:
                        time.sleep(self.request_delay)  # é¿å…è¯·æ±‚è¿‡å¿«
                        media_info = self.note_parser.parse(note_url) if self.note_parser else None

                        if media_info:
                            # å°† MediaInfo è½¬æ¢ä¸ºç¬”è®°æ•°æ®æ ¼å¼
                            note_data = self._media_info_to_note_dict(media_info, card)
                            detailed_notes.append(note_data)
                            extracted_count += 1
                            self.log_info(f"  âœ“ æˆåŠŸè§£æç¬”è®°è¯¦æƒ…: {media_info.title}")
                        else:
                            # é™çº§ï¼šä½¿ç”¨å¡ç‰‡æ•°æ®
                            detailed_notes.append(self._note_card_to_dict(card, author_info))
                            extracted_count += 1
                            self.log_info(f"  âš  æ— æ³•è§£æè¯¦æƒ…ï¼Œä½¿ç”¨å¡ç‰‡æ•°æ®")
                    except Exception as e:
                        self.log_error(f"  âœ— è§£æç¬”è®°è¯¦æƒ…å¤±è´¥: {e}")
                        # é™çº§ï¼šä½¿ç”¨å¡ç‰‡æ•°æ®
                        detailed_notes.append(self._note_card_to_dict(card, author_info))
                        extracted_count += 1
                else:
                    # ä¸è·å–è¯¦æƒ…ï¼Œç›´æ¥ä½¿ç”¨å¡ç‰‡æ•°æ®
                    detailed_notes.append(self._note_card_to_dict(card, author_info))
                    extracted_count += 1

            result = {
                "author_profile": author_info,
                "notes": detailed_notes,
                "total_notes": total_notes_count,
                "extracted_notes": extracted_count,
                "has_more": max_notes and extracted_count >= max_notes,
                "extraction_stats": {
                    "cards_found": len(note_cards),
                    "successfully_parsed": sum(1 for n in detailed_notes if n.get("has_detail")),
                    "fallback_to_cards": sum(1 for n in detailed_notes if not n.get("has_detail"))
                }
            }

            self.log_info(f"æå–å®Œæˆ: å…± {extracted_count} æ¡ç¬”è®°")

            return XiaohongshuExtractResult(
                success=True,
                result_type="author_notes",
                data=result
            )

        except Exception as e:
            self.log_error(f"æå–åšä¸»ç¬”è®°å¤±è´¥: {str(e)}")
            import traceback
            self.log_error(traceback.format_exc())
            return XiaohongshuExtractResult(
                success=False,
                result_type="author_notes",
                error_message=str(e)
            )

    def _extract_user_id_from_profile_url(self, url: str) -> Optional[str]:
        """ä»ç”¨æˆ·ä¸»é¡µURLä¸­æå–ç”¨æˆ·ID"""
        patterns = [
            r"/user/profile/([a-f0-9]+)",
            r"user/profile/([a-f0-9]+)",
        ]

        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)

        return None

    def _get_html(self, url: str) -> Optional[str]:
        """è·å–ç½‘é¡µHTMLå†…å®¹"""
        if not httpx:
            raise NetworkError("éœ€è¦å®‰è£… httpx åº“")

        try:
            with httpx.Client(headers=self.headers, timeout=30, follow_redirects=True) as client:
                response = client.get(url)
                response.raise_for_status()
                return response.text
        except httpx.HTTPError as e:
            self.log_error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return None

    def _parse_user_page_html(self, html: str, user_id: str, url: str) -> Optional[Dict]:
        """è§£æç”¨æˆ·é¡µé¢HTMLï¼Œæå–ç”¨æˆ·ä¿¡æ¯å’Œç¬”è®°å¡ç‰‡"""
        try:
            # æå–window.__INITIAL_STATE__
            initial_state_pattern = re.compile(r'window\.__INITIAL_STATE__\s*=\s*(.+?)(?=</script>)', re.DOTALL)
            initial_state_match = initial_state_pattern.search(html)

            if not initial_state_match:
                self.log_error("æœªæ‰¾åˆ° window.__INITIAL_STATE__")
                return None

            initial_state_str = initial_state_match.group(1).strip()
            if initial_state_str.endswith(';'):
                initial_state_str = initial_state_str[:-1]

            # ä¿®å¤å¹¶è§£æJSON
            initial_state_str = re.sub(r'\bundefined\b', 'null', initial_state_str)
            initial_state_str = re.sub(r',(\s*[}\]])', r'\1', initial_state_str)
            initial_state = json.loads(initial_state_str)

            # æå–ç”¨æˆ·ä¿¡æ¯
            user_data = initial_state.get("user", {})
            user_page_data = user_data.get("userPageData", {})
            basic_info = user_page_data.get("basicInfo", {})
            interactions = user_page_data.get("interactions", [])

            author_info = {
                "user_id": user_id,
                "nickname": basic_info.get("nickname", ""),
                "avatar_url": basic_info.get("images") or basic_info.get("imageb", ""),
                "xiaohongshu_id": basic_info.get("redId", ""),
                "ip_location": basic_info.get("ipLocation", ""),
                "signature": basic_info.get("desc", ""),
                "followers_count": self._parse_count(interactions, "fans"),
                "following_count": self._parse_count(interactions, "follows"),
                "total_likes_received": self._parse_count(interactions, "interaction"),
                "profile_url": url
            }

            # æå–ç¬”è®°å¡ç‰‡
            user_notes = user_data.get("notes", [])
            note_cards = []

            if user_notes and len(user_notes) > 0:
                # user_notes[0] æ˜¯å®é™…çš„ç¬”è®°åˆ—è¡¨
                notes_list = user_notes[0] if isinstance(user_notes[0], list) else []
                self.log_info(f"ä» user.notes[0] æ‰¾åˆ° {len(notes_list)} æ¡ç¬”è®°")

                for item in notes_list:
                    if isinstance(item, dict):
                        note_card = item.get("noteCard")
                        if note_card:
                            card_data = {
                                "note_id": note_card.get("noteId", ""),
                                "title": note_card.get("displayTitle", ""),
                                "type": note_card.get("type", "normal"),
                                "cover_url": note_card.get("cover", {}).get("urlDefault", ""),
                                "xsec_token": note_card.get("xsecToken", ""),
                                "liked_count": self._safe_int(note_card.get("interactInfo", {}).get("likedCount")),
                                "user_id": note_card.get("user", {}).get("userId", ""),
                                "user_nickname": note_card.get("user", {}).get("nickname", ""),
                                "user_avatar": note_card.get("user", {}).get("avatar", ""),
                            }
                            note_cards.append(card_data)

            return {
                "author_info": author_info,
                "note_cards": note_cards,
                "total_notes_count": len(note_cards)
            }

        except json.JSONDecodeError as e:
            self.log_error(f"JSONè§£æå¤±è´¥: {e}")
            return None
        except Exception as e:
            self.log_error(f"è§£æç”¨æˆ·é¡µé¢å¤±è´¥: {e}")
            import traceback
            self.log_error(traceback.format_exc())
            return None

    def _parse_user_profile_html(self, html: str, user_id: str, url: str) -> Optional[Dict]:
        """è§£æç”¨æˆ·èµ„æ–™ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥è°ƒç”¨ _parse_user_page_htmlï¼‰"""
        user_data = self._parse_user_page_html(html, user_id, url)
        if user_data:
            return user_data.get("author_info")
        return None

    def _parse_count(self, interactions: List[Dict], count_type: str) -> int:
        """ä»äº’åŠ¨æ•°æ®ä¸­è§£ææ•°é‡"""
        for item in interactions:
            if item.get("type") == count_type:
                count_str = item.get("count", "0")
                if isinstance(count_str, str):
                    # å¤„ç† "10+" è¿™ç§æ ¼å¼
                    count_str = count_str.replace("+", "")
                return self._safe_int(count_str)
        return 0

    def _safe_int(self, value) -> int:
        """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•´æ•°"""
        if value is None:
            return 0
        if isinstance(value, int):
            return value
        if isinstance(value, str):
            try:
                return int(value.replace("+", "").replace("10+", "10"))
            except ValueError:
                return 0
        if isinstance(value, float):
            return int(value)
        return 0

    def _note_card_to_dict(self, card: Dict, author_info: Dict) -> Dict:
        """å°†ç¬”è®°å¡ç‰‡è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "note_id": card.get("note_id"),
            "title": card.get("title"),
            "type": card.get("type"),
            "author": {
                "user_id": author_info.get("user_id"),
                "nickname": author_info.get("nickname"),
                "avatar_url": author_info.get("avatar_url")
            },
            "cover_image": {
                "url": card.get("cover_url")
            },
            "interaction_stats": {
                "like_count": card.get("liked_count", 0)
            },
            "source_url": f"{self.base_url}/explore/{card.get('note_id')}" if card.get("note_id") else "",
            "has_detail": False
        }

    def _media_info_to_note_dict(self, media_info: MediaInfo, card: Dict) -> Dict:
        """å°† MediaInfo è½¬æ¢ä¸ºç¬”è®°å­—å…¸"""
        # å®‰å…¨åœ°è·å– media_type çš„å€¼ï¼ˆå¤„ç†æšä¸¾å’Œå­—ç¬¦ä¸²ä¸¤ç§æƒ…å†µï¼‰
        media_type_value = media_info.media_type.value if hasattr(media_info.media_type, 'value') else str(media_info.media_type)

        return {
            "note_id": media_info.note_id or card.get("note_id"),
            "title": media_info.title or card.get("title"),
            "content": media_info.description or "",
            "type": media_type_value,
            "author": {
                "user_id": card.get("user_id"),
                "nickname": media_info.author or card.get("user_nickname"),
                "avatar_url": card.get("user_avatar")
            },
            "images": [{"url": url} for url in (media_info.download_urls.images or [])],
            "videos": [{"url": url} for url in (media_info.download_urls.video or [])],
            "cover_image": {"url": media_info.cover_url or card.get("cover_url")},
            "interaction_stats": {
                "like_count": media_info.like_count or card.get("liked_count", 0),
                "collect_count": media_info.collect_count or 0,
                "comment_count": media_info.comment_count or 0,
                "share_count": media_info.share_count or 0,
                "view_count": media_info.view_count or 0
            },
            "tags": media_info.tags or [],
            "publish_time": media_info.publish_time.isoformat() if media_info.publish_time else None,
            "source_url": media_info.url,
            "has_detail": True,
            "has_live_photo": media_info.has_live_photo
        }

    def _note_to_media_info(self, note_data: Dict, url: str) -> Optional[MediaInfo]:
        """å°†ç¬”è®°æ•°æ®è½¬æ¢ä¸º MediaInfo"""
        try:
            download_urls = DownloadUrls()

            # å¤„ç†å›¾ç‰‡
            for img in note_data.get("images", []):
                if isinstance(img, dict) and img.get("url"):
                    download_urls.images.append(img["url"])
                elif isinstance(img, str):
                    download_urls.images.append(img)

            # å¤„ç†è§†é¢‘
            for video in note_data.get("videos", []):
                if isinstance(video, dict) and video.get("url"):
                    download_urls.video.append(video["url"])
                elif isinstance(video, str):
                    download_urls.video.append(video)

            # ç¡®å®šåª’ä½“ç±»å‹
            media_type = MediaType.IMAGE
            if download_urls.video:
                media_type = MediaType.VIDEO
            elif note_data.get("has_live_photo"):
                media_type = MediaType.LIVE_PHOTO

            # äº’åŠ¨æ•°æ®
            interaction_stats = note_data.get("interaction_stats", {})

            return MediaInfo(
                platform=Platform.XIAOHONGSHU,
                title=note_data.get("title", ""),
                author=note_data.get("author", {}).get("nickname", ""),
                media_type=media_type,
                note_id=note_data.get("note_id"),
                url=url,
                download_urls=download_urls,
                description=note_data.get("content", ""),
                tags=note_data.get("tags", []),
                resource_count=len(download_urls.images) + len(download_urls.video),
                cover_url=note_data.get("cover_image", {}).get("url") or download_urls.images[0] if download_urls.images else None,
                has_live_photo=note_data.get("has_live_photo", False),
                like_count=interaction_stats.get("like_count", 0),
                collect_count=interaction_stats.get("collect_count", 0),
                comment_count=interaction_stats.get("comment_count", 0),
                share_count=interaction_stats.get("share_count", 0),
                view_count=interaction_stats.get("view_count", 0)
            )
        except Exception as e:
            self.log_error(f"è½¬æ¢ç¬”è®°æ•°æ®å¤±è´¥: {e}")
            return None

    # æ—¥å¿—æ–¹æ³•
    def log_info(self, message: str):
        if self.logger:
            self.logger.info(f"[XiaohongshuEnhanced] {message}")
        else:
            print(f"INFO: {message}")

    def log_warning(self, message: str):
        if self.logger:
            self.logger.warning(f"[XiaohongshuEnhanced] {message}")
        else:
            print(f"WARNING: {message}")

    def log_error(self, message: str):
        if self.logger:
            self.logger.error(f"[XiaohongshuEnhanced] {message}")
        else:
            print(f"ERROR: {message}")

    def log_debug(self, message: str):
        if self.logger:
            self.logger.debug(f"[XiaohongshuEnhanced] {message}")
        else:
            print(f"DEBUG: {message}")


# ä¾¿æ·å‡½æ•°
def extract_xiaohongshu_note_sync(url: str) -> XiaohongshuExtractResult:
    """åŒæ­¥ç‰ˆæœ¬ï¼šæå–å°çº¢ä¹¦ç¬”è®°ä¿¡æ¯"""
    parser = XiaohongshuEnhancedParser()

    # ä½¿ç”¨å•ä¸ªç¬”è®°è§£æå™¨
    if parser.note_parser:
        try:
            media_info = parser.note_parser.parse(url)
            if media_info:
                # å®‰å…¨åœ°è·å– media_type çš„å€¼ï¼ˆå¤„ç†æšä¸¾å’Œå­—ç¬¦ä¸²ä¸¤ç§æƒ…å†µï¼‰
                media_type_value = media_info.media_type.value if hasattr(media_info.media_type, 'value') else str(media_info.media_type)

                return XiaohongshuExtractResult(
                    success=True,
                    result_type="note",
                    data={
                        "note_id": media_info.note_id,
                        "title": media_info.title,
                        "content": media_info.description or "",  # æ·»åŠ æè¿°å­—æ®µ
                        "author": {"nickname": media_info.author},
                        "images": [{"url": i} for i in (media_info.download_urls.images or [])],
                        "videos": [{"url": v} for v in (media_info.download_urls.video or [])],
                        "live_photos": [{"url": l} for l in (media_info.download_urls.live or [])],
                        "interaction_stats": {
                            "like_count": media_info.like_count,
                            "collect_count": media_info.collect_count,
                            "comment_count": media_info.comment_count,
                            "share_count": media_info.share_count
                        },
                        "media_type": media_type_value,
                        "source_url": url
                    }
                )
        except Exception as e:
            return XiaohongshuExtractResult(
                success=False,
                result_type="note",
                error_message=str(e)
            )

    return XiaohongshuExtractResult(
        success=False,
        result_type="note",
        error_message="ç¬”è®°è§£æåŠŸèƒ½ä¸å¯ç”¨"
    )


def extract_xiaohongshu_author_sync(url: str, cookie: str = None) -> XiaohongshuExtractResult:
    """åŒæ­¥ç‰ˆæœ¬ï¼šæå–å°çº¢ä¹¦åšä¸»èµ„æ–™

    Args:
        url: ç”¨æˆ·ä¸»é¡µURL
        cookie: å°çº¢ä¹¦ Cookieï¼ˆå¯é€‰ï¼‰
    """
    parser = XiaohongshuEnhancedParser(cookie=cookie)
    return parser.parse_author_profile_sync(url)


def extract_xiaohongshu_author_notes_sync(
    url: str,
    max_notes: int = None,
    fetch_detail: bool = True,
    cookie: str = None
) -> XiaohongshuExtractResult:
    """åŒæ­¥ç‰ˆæœ¬ï¼šæå–å°çº¢ä¹¦åšä¸»æ‰€æœ‰ç¬”è®°

    Args:
        url: ç”¨æˆ·ä¸»é¡µURL
        max_notes: æœ€å¤§æå–ç¬”è®°æ•°ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
        fetch_detail: æ˜¯å¦è·å–æ¯æ¡ç¬”è®°çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬é«˜æ¸…å›¾ç‰‡/è§†é¢‘ï¼‰
        cookie: å°çº¢ä¹¦ Cookieï¼ˆæ¨èæä¾›ï¼Œç”¨äºè·å–å®Œæ•´ç¬”è®°ä¿¡æ¯ï¼‰

    æ³¨æ„ï¼š
        è·å–å®Œæ•´çš„ç¬”è®°ä¿¡æ¯ï¼ˆé«˜æ¸…å›¾ç‰‡ã€è§†é¢‘ä¸‹è½½é“¾æ¥ï¼‰éœ€è¦æä¾› Cookie
        è·å–æ–¹å¼ï¼šæµè§ˆå™¨ F12 -> Network -> å¤åˆ¶ Request Header ä¸­çš„ Cookie
    """
    parser = XiaohongshuEnhancedParser(cookie=cookie)
    return parser.parse_author_notes_sync(url, max_notes=max_notes, fetch_detail=fetch_detail)
